{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3a6e9b9",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook illustrates some basic document handling using [Spacy] (https://spacy.io/). Spacy is fast, and powerful, but not completely trivial to understand. There are though lots of useful resources, and the documentation is excellent.\n",
    "\n",
    "**The first block of our code simply sets things up - most important here is the language model that we use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5a2314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy #Our NLP tools\n",
    "import math\n",
    "from collections import Counter #We will use this to do simple counts of terms\n",
    "\n",
    "#Load a German language model to do German NLP - the models we use will influence our results a lot\n",
    "#nlp = spacy.load('de_core_news_md')\n",
    "#And an English language model for English\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7881385",
   "metadata": {},
   "source": [
    "The following function calculates term frequency for a given document. It does so for lemmatised tokens, and ignores stop words and punctuation. You can try changing the terms for which frequency is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7aeb185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(text):\n",
    "    doc = nlp(text)\n",
    "    n = len(doc)\n",
    "    \n",
    "    terms = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct ]\n",
    "    tf = dict(Counter(terms))\n",
    "    \n",
    "    for term, count in tf.items():\n",
    "        tf[term] = count/n\n",
    "        \n",
    "    return tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf092ae",
   "metadata": {},
   "source": [
    "This function calculates document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e7f1223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df(texts):\n",
    "    df = dict()\n",
    "    ndocs = len(texts)\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        terms = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct ]\n",
    "        tf = set(terms)\n",
    "        for t in tf:\n",
    "            if t in df:\n",
    "                count = df[t]\n",
    "                count = count + 1\n",
    "            else:\n",
    "                count = 1\n",
    "            df[t] = count/ndocs\n",
    "    for term, count in df.items():\n",
    "        df[term] = math.log10(ndocs/(count + 1))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568649a3",
   "metadata": {},
   "source": [
    "Now we take a corpus, and calculate tf-idf scores for every term in every document in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eec5dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(corpus):\n",
    "    idf = df(list(corpus.values()))\n",
    "    results = {}\n",
    "    for id, text in corpus.items():                \n",
    "        t = tf(text)\n",
    "        scores = {}\n",
    "        for term in t:\n",
    "            scores[term] = t[term]*idf[term]\n",
    "        results[id] = scores\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c5cc8d",
   "metadata": {},
   "source": [
    "Finally, we are ready to do a search. We calculate scores for each document based on the query and tfidf scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2108bbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleSearch(query, weights):\n",
    "    q = nlp(query)\n",
    "    \n",
    "    #Iterate through each document and add its tf-idf score\n",
    "    results = {}\n",
    "    for id, scores in weights.items():\n",
    "        score = 0\n",
    "        for token in q:\n",
    "            if token.lemma_ in scores:\n",
    "                score = score + scores[token.lemma_]\n",
    "        results[id] = score\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40946bb",
   "metadata": {},
   "source": [
    "You can experiment with different corpora, and different queries to see how the results vary. Ranks for results would be based on tfidf scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2204d468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0.10045923649826893, 2: 0.20091847299653787, 3: 0.24110216759584546, 4: 0.12055108379792273, 5: 0}\n"
     ]
    }
   ],
   "source": [
    "corpus = {1:\"the cat sat on the mat\",\n",
    "          2:\"the dog played with the cat\",\n",
    "          3:\"the cat bit the dog\",\n",
    "          4:\"the boy bit the dog\",\n",
    "          5:\"the girl saw the boy far away\"}\n",
    "\n",
    "weights = tfidf(corpus)\n",
    "\n",
    "results = simpleSearch('cat dog', weights)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e628f22e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4db7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
