{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3a6e9b9",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook illustrates some basic document handling using [Spacy] (https://spacy.io/). Spacy is fast, and powerful, but not completely trivial to understand. There are though lots of useful resources, and the documentation is excellent.\n",
    "\n",
    "**The first block of our code simply sets things up - most important here is the language model that we use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a2314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy #Our NLP tools\n",
    "from collections import Counter #We will use this to do simple counts of terms\n",
    "\n",
    "#Load a German language model to do NLP - the models we use will influence our results a lot\n",
    "nlp = spacy.load('de_core_news_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7881385",
   "metadata": {},
   "source": [
    "Now we load a default list of stop words and print them out.\n",
    "\n",
    "Look through the list of stop words, and consider what issues they might cause if we are interested in spatial relationships?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c842f81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block loads our stop words \n",
    "stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "print(len(stopwords))\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dc846e",
   "metadata": {},
   "source": [
    "Our first block shows how Spacy can read and process a single sentence. We look here at some different outputs, in particular:\n",
    "\n",
    "- Simple tokens: how Spacy breaks up the document into terms\n",
    "- Lemmas: processing of the tokens reducing them to canonical forms \n",
    "- Parts of speech: labelling each token with a part of speech\n",
    "- Stop words: finding tokens that are included in our stopword list\n",
    "\n",
    "Look at the results, and compare the differences between tokens and lemmas. Look at the part of speech tags - do they make sense? How many stop words do we find in the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acc01eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#First we demonstrate how the NLP works for a few sentences\n",
    "text = \"Von der Grimsel verfolgt man erst den Weg nach der Handeck bis zum Kunzentannli, hier verlässt man den Grimselweg und steigt sachte über Hinter-Stock und Gelmer Gassle auf schmalem Kuhpfade bergan zum Seemätteli. Nach einer Viertelstunde rauhen Steigens, theilweise über glattpolirte, helle, oft tief gefurchte Granitfelsen, bei deren Anblick man kaum begreift, wie Kühe da hinauf getrieben werden können, gewinnt man die Höhe des Seebodens und schaut entzückt den etwa 200 Fuss tief zu seinen Fussen liegenden, die ganze Thalbreite abschliessenden, friedlichen See, der die steil abfällenden Felswände der Gelmer Hörner und des Schaubhorns bespült und dessen Abfluss als Gelmer Bach in zierlichem Sturze in das Hasli-Thal herunterbraust. Ein schmaler felsiger Pfad führt um den See herum, dessen hinterer seichter Theil auf halb im Sumpfe versenkten Steinen, bei grösserem Wasserreichthum watend, passirt werden muss. Nach Ueberschreitung eines älteren Felsbruches, dessen mächtige Trümmer in wildem Chaos durch und übereinander liegen, gelangt man nach 2Stunden Marsch auf die einsame steinreiche Gelmer Alp, wo sich der Reisende einer freundlichen Aufnahme zu erfreuen hat.\"\n",
    "\n",
    "doc = nlp(text) #Load and process a document using Spacy - here we do all the work annotating the text with lots of information\n",
    "\n",
    "#Do some pretty printing of the results\n",
    "print(f\"{'Token':<20}\\t{'Lemma':<20}\\t{'POS':<6}\\t{'Stop':<5}\\n\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<20}\\t{token.lemma_:<20}\\t{token.pos_:<6}\\t{token.is_stop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e66fdbf",
   "metadata": {},
   "source": [
    "Let's look and see what nouns were in our document - we can simply iterate through the tokens, keeping those identified as nouns.\n",
    "\n",
    "Do these all make sense? Where do you see issues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce29096",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = (token for token in doc if token.pos_ == 'NOUN')\n",
    "\n",
    "for noun in nouns:\n",
    "    print(f\"{noun}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c99508",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "terms = [token.text for token in doc]\n",
    "ranked_terms = Counter(terms).most_common()\n",
    "\n",
    "print(f\"{'Term':<20}\\t{'Count':<3}\\n\")\n",
    "for term, count in ranked_terms:\n",
    "    print(f\"{term:<20}\\t{count:<3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f300b16f",
   "metadata": {},
   "source": [
    "I've prepared a directory with a few Text+Berg documents as a small corpus. This code loads three random documents from the 20 possible ones, and runs them through Spacy's NLP pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6002a93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk\n",
    "import random\n",
    "\n",
    "path = './data/textBergSample/'\n",
    "filenames = next(walk(path), (None, None, []))[2]\n",
    "files = random.sample(filenames, 3)\n",
    "\n",
    "for file in files:\n",
    "    with open(path + file, 'r', encoding='utf-8') as file:\n",
    "        text = text + file.read().replace('\\n', ' ')\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e8b61f",
   "metadata": {},
   "source": [
    "This code analyses the frequency of different tokens in our random collection. There are different conditions prepared here. Change these and comment on the properties of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d065ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "terms = [token.text for token in doc]\n",
    "\n",
    "# Remove stop words and punctuation\n",
    "#terms = [token.text for token in doc if not token.is_stop and not token.is_punct] \n",
    "\n",
    "# Remove stop words, punctuation and retain only nouns\n",
    "#terms = [token.text for token in doc if not token.is_stop and not token.is_punct and token.pos_ == 'NOUN']\n",
    "\n",
    "ranked_terms = Counter(terms).most_common(20)\n",
    "\n",
    "print(f\"{'Term':<20}\\t{'Count':<3}\\n\")\n",
    "for term, count in ranked_terms:\n",
    "    print(f\"{term:<20}\\t{count:<3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f547811",
   "metadata": {},
   "source": [
    "We can also use Spacy to perform Named Entity Recognition (NER). Spacy identifies named entities and labels them, for example as LOC (location), ORG (organisation) or PER (person). It's important to understand that NER (like other elements of NLP) is not perfect. \n",
    "\n",
    "**Calculate the precision and recall for the text passage processed here.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aec3d7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = \"Von der Grimsel verfolgt man erst den Weg nach der Handeck bis zum Kunzentannli, hier verlässt man den Grimselweg und steigt sachte über Hinter-Stock und Gelmer Gassle auf schmalem Kuhpfade bergan zum Seemätteli. Nach einer Viertelstunde rauhen Steigens, theilweise über glattpolirte, helle, oft tief gefurchte Granitfelsen, bei deren Anblick man kaum begreift, wie Kühe da hinauf getrieben werden können, gewinnt man die Höhe des Seebodens und schaut entzückt den etwa 200 Fuss tief zu seinen Fussen liegenden, die ganze Thalbreite abschliessenden, friedlichen See, der die steil abfällenden Felswände der Gelmer Hörner und des Schaubhorns bespült und dessen Abfluss als Gelmer Bach in zierlichem Sturze in das Hasli-Thal herunterbraust. Ein schmaler felsiger Pfad führt um den See herum, dessen hinterer seichter Theil auf halb im Sumpfe versenkten Steinen, bei grösserem Wasserreichthum watend, passirt werden muss. Nach Ueberschreitung eines älteren Felsbruches, dessen mächtige Trümmer in wildem Chaos durch und übereinander liegen, gelangt man nach 2Stunden Marsch auf die einsame steinreiche Gelmer Alp, wo sich der Reisende einer freundlichen Aufnahme zu erfreuen hat.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "#Pretty output of the NER results\n",
    "spacy.displacy.render(doc, style=\"ent\")\n",
    "\n",
    "# Iterate through the named entities found, and their types. \n",
    "print(f\"{'Token':<20}\\t{'Type':<3}\\n\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text:<20}\\t{ent.label_:<3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16651b63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
